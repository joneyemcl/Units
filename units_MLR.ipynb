{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458d825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2758448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "\n",
    "file_path = '2024_03_numunit_withOMOPtarget.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "# Separate the data into training and prediction datasets\n",
    "train_data = data.dropna(subset=['source_code_description']).copy()\n",
    "predict_data = data[data['source_code_description'].isna()].copy()\n",
    "\n",
    "# Drop rows with NaN values in 'description' column from the training data\n",
    "train_data.dropna(subset=['description'], inplace=True)\n",
    "predict_data.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "# drop 'percent' and 'score' rows from `source_code_description` in training data\n",
    "train_data = train_data[~train_data['source_code_description'].isin(['percent', 'score',])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee58cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_sample = train_data.sample(n=200, random_state=1)\\ntrain_sample.to_csv('processing/train_sample_large.csv', index=False)\\n\\npred_sample = predict_data.sample(n=200, random_state=1)\\npred_sample.to_csv('processing/pred_sample.csv', index=False)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting train and test data\n",
    "\n",
    "\"\"\"\n",
    "print(train_data.info(verbose=True)) # looks fine; has correct amount of rows\n",
    "print(predict_data.info(verbose=True)) # looks fine; has correct amount of rows\n",
    "\"\"\"\n",
    "\n",
    "# exporting samples of the train and test data\n",
    "\"\"\"\n",
    "train_sample = train_data.sample(n=200, random_state=1)\n",
    "train_sample.to_csv('processing/train_sample_large.csv', index=False)\n",
    "\n",
    "pred_sample = predict_data.sample(n=200, random_state=1)\n",
    "pred_sample.to_csv('processing/pred_sample.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b7c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding feature and label layers\n",
    "\n",
    "# Label encode the target variable for training data\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['target'] = label_encoder.fit_transform(train_data['source_code_description'])\n",
    "\n",
    "# Vectorize the 'description' column using Count Vectorizer for training data\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(train_data['description'])\n",
    "\n",
    "# Extract the target variable for training data\n",
    "y_train = train_data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f751ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Get the list of all tokens found in the data\\nvocabulary = vectorizer.vocabulary_\\ntokens = list(vocabulary.keys())\\n\\n# Sort tokens by their index to make it more readable\\nsorted_tokens = sorted(tokens, key=lambda x: vocabulary[x])\\n\\n# Print the sorted list of tokens\\nprint(sorted_tokens)\\n\\n# If you want to save the tokens to a file for further inspection\\nwith open(\\'processing/tokens_list.csv\\', \\'w\\') as f:\\n    for token in sorted_tokens:\\n        f.write(f\"{token}\\n\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the feature and label layers\n",
    "\n",
    "# viewing X_train\n",
    "\"\"\"\n",
    "copied_X_train = X_train.copy()\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix (numpy array)\n",
    "dense_matrix = copied_X_train.toarray()\n",
    "\n",
    "# Create a DataFrame from the dense matrix\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "csv_file_path = 'processing/sparse_matrix_dense.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f'Sparse matrix exported as dense matrix to {csv_file_path}')\n",
    "\"\"\"\n",
    "\n",
    "# viewing list of tokens found during vectorization\n",
    "\n",
    "\"\"\"\n",
    "# Get the list of all tokens found in the data\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "tokens = list(vocabulary.keys())\n",
    "\n",
    "# Sort tokens by their index to make it more readable\n",
    "sorted_tokens = sorted(tokens, key=lambda x: vocabulary[x])\n",
    "\n",
    "# Print the sorted list of tokens\n",
    "print(sorted_tokens)\n",
    "\n",
    "# If you want to save the tokens to a file for further inspection\n",
    "with open('processing/tokens_list.csv', 'w') as f:\n",
    "    for token in sorted_tokens:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0373b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "\n",
    "solver = 'newton-cg'   \n",
    "C = 0.01          \n",
    "penalty = 'l2'    \n",
    "max_iter = 10000   \n",
    "multi_class = 'multinomial'   \n",
    "class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9717cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a multinomial logistic regression model\n",
    "model = LogisticRegression(\n",
    "    solver=solver,\n",
    "    C=C,\n",
    "    penalty=penalty,\n",
    "    max_iter=max_iter,\n",
    "    multi_class=multi_class,\n",
    "    class_weight=class_weight \n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vectorize the 'description' column for prediction data\n",
    "X_predict = vectorizer.transform(predict_data['description'])\n",
    "\n",
    "# Predict the 'source_code_description' for the prediction data\n",
    "y_predict = model.predict(X_predict)\n",
    "predict_data['predicted_source_code_description'] = label_encoder.inverse_transform(y_predict)\n",
    "\n",
    "# Combine the results back into the original dataset for comparison or further analysis\n",
    "result_data = pd.concat([train_data, predict_data], ignore_index=True)\n",
    "\n",
    "# Display the predictions for the missing data\n",
    "#print(predict_data[['description', 'predicted_source_code_description']])\n",
    "\n",
    "# If you want to save the results to a new CSV file\n",
    "result_data.to_csv('processing/predicted_2024_03_numunit_withOMOPtarget.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
